{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import glob\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nilearn import datasets\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nilearn.image import resample_to_img\n",
    "from factor_analyzer import FactorAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_figures = True\n",
    "PATH = \"...\"\n",
    "scaler = StandardScaler()\n",
    "imp = SimpleImputer(missing_values=999,strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the colors\n",
    "color = ['#3f005c', '#ffa600']\n",
    "my_colors = []\n",
    "for c in color:\n",
    "    my_colors.append(matplotlib.colors.to_rgb(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lesion load data\n",
    "lesion_load = pd.read_csv(PATH + \"data.csv\")\n",
    "lesion_load = lesion_load.drop(columns=\"Unnamed: 0\")\n",
    "lesion_bilat = lesion_load.iloc[:1401].copy()\n",
    "lesion_bilat = lesion_bilat.rename(columns={\"0\": \"Left\"})\n",
    "lesion_bilat[\"Right\"] = np.array(lesion_load.iloc[1401:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load behavioral data\n",
    "Behav = pd.read_excel(PATH + 'behavioral.xlsx')\n",
    "Behav = Behav[np.invert((lesion_bilat!=0).all(axis=1))]  # excluding patients with bilateral lesions\n",
    "# Imputing missing values with median\n",
    "Behav_imp = pd.DataFrame(imp.fit_transform(Behav))\n",
    "Behav_imp.columns = Behav.columns\n",
    "# Average time for TMT A and B\n",
    "Behav_imp['TMT_Time_zscore_neg'] = np.mean([Behav_imp.TMT_A_Time_zscore_neg, Behav_imp.TMT_B_Time_zscore_neg], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra behavioral measures\n",
    "Behav_extra = pd.read_excel(PATH + 'behav_extended.xlsx')\n",
    "Behav_extra = Behav_extra[Behav_extra.ID.isin(Behav_imp.ID)]\n",
    "Behav_extra_imp = pd.DataFrame(imp.fit_transform(Behav_extra))\n",
    "Behav_extra_imp.columns = Behav_extra.columns\n",
    "Behav_imp['SemanticFluency'] = scaler.fit_transform(Behav_extra_imp.Semanticfluencyanimal.values.reshape(-1, 1))\n",
    "Behav_imp['PhonemicFluency'] = scaler.fit_transform(Behav_extra_imp.Phonemicfluency_total.values.reshape(-1, 1))\n",
    "Behav_imp['DigitSymbolCoding'] = scaler.fit_transform(Behav_extra_imp.DigitSymbolCoding_correct.values.reshape(-1, 1))\n",
    "Behav_imp = Behav_imp.drop(columns=[\"ID\",\"Cohort\"])\n",
    "# Scaling the total infarct volume\n",
    "Total_infarct_volume_scaled = scaler.fit_transform(np.array(Behav_imp.Total_infarct_volume).reshape(-1, 1))\n",
    "Behav_imp[\"Total_infarct_volume_scaled\"] = Total_infarct_volume_scaled\n",
    "# Collapsing the data\n",
    "Behav_imp_collapsed = pd.concat([Behav_imp, Behav_imp], axis=0)\n",
    "Behav_imp_collapsed.index = range(0,2160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lesion masks\n",
    "subject_names = glob.glob(PATH + '/lesionmaps/*',)\n",
    "subject_names = sorted(subject_names)\n",
    "# Load atlas\n",
    "dataset = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr50-2mm', symmetric_split=True)\n",
    "cortical_mask = dataset.maps\n",
    "cortical_labels = dataset.labels\n",
    "dataset = datasets.fetch_atlas_harvard_oxford('sub-maxprob-thr50-2mm')\n",
    "subcortical_mask = dataset.maps\n",
    "subcortical_labels = dataset.labels\n",
    "# Edit the labels\n",
    "cortical_labels = pd.DataFrame(cortical_labels)\n",
    "cortical_labels = cortical_labels.drop(index=[0,93,94])\n",
    "subcortical_labels = pd.DataFrame(subcortical_labels)\n",
    "subcortical_labels = subcortical_labels.drop(index=[0,1,2,3,12,13,14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading mapped to cortical and subcortical regions\n",
    "cortical = pd.read_csv(PATH + \"/cortical.csv\")\n",
    "cortical = cortical.drop(columns=\"Unnamed: 0\")\n",
    "subcortical = pd.read_csv(PATH + \"/subcortical.csv\")\n",
    "subcortical = subcortical.drop(columns=\"Unnamed: 0\")\n",
    "\n",
    "# resample one sub to the cortical mask\n",
    "cortical_resampled_stat_img = resample_to_img(cortical_mask, subject_names[0], interpolation='nearest')\n",
    "# resample one subject to subcortical mask\n",
    "subcortical_resampled_stat_img = resample_to_img(subcortical_mask, subject_names[0], interpolation='nearest')\n",
    "# count voxels in a region\n",
    "cortical_count_labels = np.bincount(cortical_resampled_stat_img.get_data().astype(np.int64).ravel())\n",
    "subcortical_count_labels = np.bincount(subcortical_resampled_stat_img.get_data().astype(np.int64).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the labels\n",
    "cortical_count_labels = pd.DataFrame(cortical_count_labels)\n",
    "cortical_count_labels = np.array(cortical_count_labels.drop(index=[0,93,94])).ravel()\n",
    "subcortical_count_labels = pd.DataFrame(subcortical_count_labels)\n",
    "subcortical_count_labels = np.array(subcortical_count_labels.drop(index=[0, ])).ravel()\n",
    "\n",
    "# number of lesion in each region\n",
    "cortical_lesion_count_absolute = cortical*cortical_count_labels\n",
    "subcortical_lesion_count_absolute = subcortical*subcortical_count_labels\n",
    "subcortical_lesion_count_absolute = subcortical_lesion_count_absolute.drop(columns=[\"0\",\"1\",\"2\",\"11\",\"12\",\"13\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns\n",
    "cortical_lesion_count_absolute.columns = cortical_labels\n",
    "subcortical_lesion_count_absolute.columns = subcortical_labels\n",
    "\n",
    "new_columns = []\n",
    "for parcel in range(len(cortical_lesion_count_absolute.columns)):\n",
    "        new_columns.append(cortical_lesion_count_absolute.columns[parcel][0].replace(\" \", \"_\"))\n",
    "cortical_lesion_count_absolute.columns = new_columns\n",
    "\n",
    "new_columns = []\n",
    "for parcel in range(0, len(subcortical_lesion_count_absolute.columns)):\n",
    "        new_columns.append(subcortical_lesion_count_absolute.columns[parcel][0].replace(\" \", \"_\"))\n",
    "subcortical_lesion_count_absolute.columns = new_columns\n",
    "\n",
    "subcortical_lesion_count_absolute = subcortical_lesion_count_absolute.drop(columns=['Brain-Stem'])\n",
    "\n",
    "# reduce to non-bilateral\n",
    "cortical_lesion_count_absolute = cortical_lesion_count_absolute[np.invert((lesion_bilat!=0).all(axis=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into left and right hemisphere\n",
    "cortical_lesion_left = cortical_lesion_count_absolute.iloc[:,range(0,94,2)]\n",
    "cortical_lesion_left.loc[:,\"Left_hemisphere\"] = 0\n",
    "cortical_lesion_right = cortical_lesion_count_absolute.iloc[:,range(1,95,2)]\n",
    "cortical_lesion_right.loc[:,\"Left_hemisphere\"] = 1\n",
    "\n",
    "for hem, hem_name in zip([cortical_lesion_right, cortical_lesion_left], [\"Right_\", \"Left_\"]):\n",
    "    new_columns = []\n",
    "    for parcel in range(0, len(hem.columns[:-1])):\n",
    "            new_columns.append(hem.columns[parcel].replace(hem_name, \"\"))\n",
    "    new_columns.append(\"Left_hemisphere\")\n",
    "    hem.columns = new_columns\n",
    "\n",
    "cortical_lesion_collapsed = pd.concat([cortical_lesion_left, cortical_lesion_right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into left and right hemisphere\n",
    "subcortical_lesion_count_absolute = subcortical_lesion_count_absolute[np.invert((lesion_bilat!=0).all(axis=1))]\n",
    "\n",
    "subcortical_lesion_left = subcortical_lesion_count_absolute.iloc[:,range(0,7,1)]\n",
    "subcortical_lesion_left.loc[:,\"Left_hemisphere\"] = 0\n",
    "subcortical_lesion_right = subcortical_lesion_count_absolute.iloc[:,range(7,14,1)]\n",
    "subcortical_lesion_right.loc[:,\"Left_hemisphere\"] = 1\n",
    "\n",
    "for hem, hem_name in zip([subcortical_lesion_right, subcortical_lesion_left], [\"Right_\", \"Left_\"]):\n",
    "    new_columns = []\n",
    "    for parcel in range(0, len(hem.columns[:-1])):\n",
    "            new_columns.append(hem.columns[parcel].replace(hem_name, \"\"))\n",
    "    new_columns.append(\"Left_hemisphere\")\n",
    "    hem.columns = new_columns\n",
    "\n",
    "subcortical_lesion_collapsed = pd.concat([subcortical_lesion_left, subcortical_lesion_right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lesions_collapsed_all = pd.concat([cortical_lesion_collapsed, subcortical_lesion_collapsed], axis=1)\n",
    "lesions_collapsed_hemisphere = lesions_collapsed_all[\"Left_hemisphere\"].copy()\n",
    "lesions_collapsed_hemisphere = np.array(lesions_collapsed_hemisphere)\n",
    "lesions_collapsed_all = lesions_collapsed_all.drop(columns=\"Left_hemisphere\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere_idx = lesions_collapsed_hemisphere[:,0]\n",
    "n_hemispheres = len(np.unique(lesions_collapsed_hemisphere))\n",
    "n_components = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age mean scaled\n",
    "Age_mean_scaled = Behav_imp[\"Age\"] - Behav_imp[\"Age\"].mean()\n",
    "Age_mean_scaled = np.concatenate((Age_mean_scaled, Age_mean_scaled))\n",
    "Age_mean_scaled_2 = np.array(Age_mean_scaled * Age_mean_scaled)\n",
    "\n",
    "Behav_imp_collapsed[\"Sex_1_m\"] = Behav_imp_collapsed[\"Sex\"].replace(1,0)\n",
    "Behav_imp_collapsed[\"Sex_1_m\"] = Behav_imp_collapsed[\"Sex_1_m\"].replace(2,1)\n",
    "Sex_m = np.int64(np.array(Behav_imp_collapsed[\"Sex_1_m\"]))\n",
    "Behav_imp_collapsed[\"Sex_1_w\"] = Behav_imp_collapsed[\"Sex\"].replace(2,0)\n",
    "Sex_w = np.int64(np.array(Behav_imp_collapsed[\"Sex_1_w\"]))\n",
    "\n",
    "Age_Sex_m = Age_mean_scaled * Sex_m\n",
    "Age_Sex_w = Age_mean_scaled * Sex_w\n",
    "\n",
    "Lesion_vol_scaled = np.array(Behav_imp_collapsed.Total_infarct_volume_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Education_mean_scaled = Behav_imp[\"Education_years\"] - Behav_imp[\"Education_years\"].mean() \n",
    "Education_mean_scaled = np.concatenate((Education_mean_scaled, Education_mean_scaled))\n",
    "Education_mean_scaled = np.array(Education_mean_scaled)\n",
    "\n",
    "IQ_mean_scaled = Behav_imp[\"IQCODE\"] - Behav_imp[\"IQCODE\"].mean()\n",
    "IQ_mean_scaled = np.concatenate((IQ_mean_scaled , IQ_mean_scaled ))\n",
    "IQ_mean_scaled = np.array(IQ_mean_scaled )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection and removal\n",
    "X1 = Behav_imp_collapsed.MMSE_total_zscore.values\n",
    "X2 = Behav_imp_collapsed.ReyComplexFigureTestCopy_zscore.values\n",
    "X3 = Behav_imp_collapsed.BostonNamingTest_zscore.values\n",
    "X4 = Behav_imp_collapsed.Seoul_Verbal_Learning_Test_immediate_recall_total_zscore.values\n",
    "X5 = Behav_imp_collapsed.TMT_Time_zscore_neg.values\n",
    "X6 = Behav_imp_collapsed.SemanticFluency.values\n",
    "X7 = Behav_imp_collapsed.PhonemicFluency.values\n",
    "X8 = Behav_imp_collapsed.DigitSymbolCoding.values\n",
    "X_all = np.array([X1, X2, X3, X4, X5, X6, X7, X8])\n",
    "\n",
    "# Input names\n",
    "cat_names = ['MiniMentalStateExam', 'ReyComplexFigure', 'BostonNaming',\n",
    "             'SeoulVerbalLearning', 'TrailMaking', 'SemanticFluency',\n",
    "             'PhonemicFluency', 'DigitSymbolCoding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factor analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_comp = 4\n",
    "fa = FactorAnalyzer(n_factors=n_comp, rotation=\"varimax\", method='principal')\n",
    "X_star = fa.fit_transform(X_all.T)\n",
    "\n",
    "A_ = fa.loadings_\n",
    "\n",
    "sns.set_theme(style=\"white\", palette='colorblind', font_scale=1.8)\n",
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "g = sns.heatmap(data=(A_), cbar=True, cmap='RdBu_r',  linewidths=0.5,\n",
    "                square=False, annot=False, center=0)\n",
    "                # vmin=0.2, vmax=0.8,)\n",
    "g.set_yticklabels(cat_names, rotation=0)\n",
    "g.set_xticklabels(np.arange(1,n_comp+1), rotation=0)\n",
    "g.set_ylabel('')\n",
    "g.set_xlabel('Dimension')\n",
    "ax.set_title('Factor loadings matrix', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 8\n",
    "fa = FactorAnalyzer(n_factors=n_comp, rotation=\"varimax\")\n",
    "X_star = fa.fit_transform(X_all.T)\n",
    "d = fa.get_factor_variance()\n",
    "ev = np.array(d[1]*100)\n",
    "eig = fa.get_eigenvalues()\n",
    "\n",
    "# Create scree plot using matplotlib\n",
    "d = pd.DataFrame({'factor': np.arange(0, len(ev)),\n",
    "                  'ev': np.cumsum(ev),\n",
    "                  'eig': eig[0]})\n",
    "sns.set_theme(style=\"white\", palette='colorblind', font_scale=2.2)\n",
    "fig, ax = plt.subplots(1, figsize=(12, 6))\n",
    "sns.barplot(x='factor',y='ev', data=d, ax=ax, color=color[0])\n",
    "plt.ylabel('Cum. expl. variance [%]', fontsize=24)\n",
    "plt.xlabel('Factors', fontsize=26)\n",
    "ax2 = ax.twinx()\n",
    "sns.scatterplot(x='factor',y='eig', data=d, ax=ax2, color=color[1], s=80)\n",
    "ax2.plot(d.factor, d.eig, c=color[1],linewidth=2.5)\n",
    "ax2.set_yticks(np.array([1, 2, 3]))\n",
    "plt.ylabel('Eigenvalues', fontsize=24)\n",
    "plt.title('', fontsize=22)\n",
    "plt.xlabel('Factors', fontsize=26)\n",
    "ax.set_xticklabels(np.arange(1, len(ev)+1))\n",
    "ax.set_yticks(np.array([20, 40, 60]))\n",
    "ax.grid()\n",
    "sns.despine(left=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pymc3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "c2c9e36c825e41c6adfda42748c2127e4d976114e16b97414f8fe9b2747b1132"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
